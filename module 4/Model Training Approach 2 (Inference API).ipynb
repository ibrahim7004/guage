{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Steps to using pre-trained inference model with inference api:\n",
    "- Go to [huggingface](https://huggingface.co/) -> Models\n",
    "- Filter the models according to your needs (By task and sub-task (NLP -> Text Classification), 'financial' + 'sentiment analysis', updated in 2024)\n",
    "- Check whether your chosen model allows inference api integration \n",
    "- **Inference Api:** is a way to use pretrained huggingface models without having to download them locally.\n",
    "- Test out the performances of different models that meet your initial criteria by using your data as input (in this case it is text from any pdf), on the inference api interface, and then checking out the output from the model.\n",
    "- Check out details of the training dataset used for that model by clicking on the training dataset provided on the model page, and view the X feature values used and the format in which they were passed (e.g. textual sentences, 2-digit numbers), and also viewing the type of output format(e.g. pos, neg, neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Evaluation Metrics:\n",
    "- Accuracy: Measures the overall correctess of the predictions made by the model. For e.g. if a model predicts 90 out of 100 data points accurately then the accuracy would be 90% (0.9).\n",
    "- Loss: This metric measures how well model performs during training. It represents the difference between the predicted values and the actual values. The goal is to minimize this.\n",
    "- Highest accuracy or lowest loss does not necessarily imply best model. The model may be overfit, testing with real-time predictions helps understand the actual performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    How to get your access_token for model:\n",
    "1) **One-time step** (Creating a way to communicate with huggingface api): go to your huggingface profile -> settings -> access tokens -> generate a new token -> set your name, keep the type set to 'Read', and click generate token. (You need not do this step again and again, can use same access token for different models, however, you may create new access tokens if you wish.)\n",
    "2) Go to 'deploy' on the model page, select inference api (serverless). It'll open up a new window -> make sure language is set to Python -> select your desired user token from 'Token' -> click on 'show API token' -> simply copy the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code that you copy will follow a standard format each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests    \n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/bert-base-uncased\"\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "data = query({\"inputs\": \"The answer to the universe is [MASK].\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- importing library requests to retrieve and communicate with web page.\n",
    "- API Url is simply the url of the model's web page.\n",
    "- Bearer is the access token you've generated and chosen to use with this model.\n",
    "- You can read up on it [here](https://huggingface.co/docs/api-inference/detailed_parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"I like you. I love you\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'neutral', 'score': 0.9996621608734131},\n",
       "  {'label': 'positive', 'score': 0.0002529192133806646},\n",
       "  {'label': 'negative', 'score': 8.493565837852657e-05}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code:\n",
    "output = query({\"inputs\": \"I like you. I love you\"})\n",
    "\n",
    "- is reponsible for passing user's prompt to the model and getting an output response.\n",
    "- The dictionary holds \"inputs\" which is the prompt that we want to pass.\n",
    "- the returned model value wil be stored to the output variable.\n",
    "- In this specifc example, we pass the text \"I like you. I love you\" to the model, and it returns a certain output which represent the label values for this specific text (prompt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "def retrieve_response(prompt):\n",
    "    output = query({\"inputs\": prompt})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative 0.9950839877128601\n",
      "positive 0.004714443814009428\n",
      "neutral 0.00020149191550444812\n"
     ]
    }
   ],
   "source": [
    "user_input= input('Enter your text for sentiment analysis:')\n",
    "response = retrieve_response(user_input)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    parsed_reponse_label = response[0][i]['label']\n",
    "    parsed_reponse_score = response[0][i]['score']\n",
    "    print(parsed_reponse_label, parsed_reponse_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
