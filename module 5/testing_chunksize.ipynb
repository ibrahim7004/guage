{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"['call', 'sell', 'believ', 'maersk', 'financi', 'perform', 'vulner', 'lower', 'freight', 'rate', 'downward', 'trend', 'due', 'continu', 'subdu', 'demand', 'eas', 'port', 'congest', 'releas', 'capac', 'back', 'furthermor', 'think', 'neg', 'feedback', 'loop', 'freight', 'rate', 'continu', 'think', 'custom', 'may', 'seek', 'renegoti', 'given', 'price', 'differenti', 'contract', 'spot', 'also', 'overhang', 'situat', 'may', 'amplifi', 'entri', 'new', 'vessel', 'project', 'increas', 'industri', 'suppli', 'nonetheless', 'posit', 'term', 'strategi', 'grow', 'logist', 'busi', 'complement', 'core', 'busi', 'upsid', 'risk', 'includ', 'strong', 'econom', 'growth', 'improv', 'ship', 'activ', 'posit', 'revers', 'freight', 'rate', 'effect', 'cost', 'target', 'price', 'maersk', 'reflect', 'broadli', 'line', 'peer', 'averag']\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "    Chunk-size Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tokens):\n",
    "    outputs = model(**tokens)   # pass tokens to get output\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1) # Convert retrieved outputs to probabilities\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1758 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50118, 134, 4, 289, 5969, 21770, 46058, 104, 3243, 248, 19739, 3063, 19713, 2371, 3808, 6034, 2808, 4307, 41603, 6390, 8640, 1889, 5758, 2606, 25815, 2118, 31299, 7111, 8662, 15231, 18827, 221, 7949, 2604, 30018, 9162, 3858, 7981, 25049, 1723, 5089, 26747, 6597, 2571, 18347, 4, 50118, 50118, 3762, 11, 292, 1791, 266, 519, 2281, 11196, 6, 8, 4583, 3608, 21416, 1446, 16, 55, 1537, 566, 3240, 3362, 6, 604, 6, 8, 6689, 12568, 4, 152, 16, 1341, 430, 31, 97, 613, 9571, 13588, 30, 5, 12612, 6078, 6, 1664, 1908, 1581, 4, 44, 48, 1185, 17, 27, 241, 164, 7, 33, 55, 4478, 867, 1174, 85, 17, 27, 29, 9861, 15, 201, 7, 11427, 6, 7, 6296, 6, 7, 9263, 2476, 963, 4, 17, 46, 50118, 50118, 14009, 13513, 34798, 271, 6, 10, 320, 3484, 8, 3080, 4589, 8, 122, 1031, 736, 9, 5, 5366, 6944, 2534, 824, 13, 2108, 5123, 6, 3307, 1130, 9588, 1503, 7, 434, 11, 2304, 21416, 35, 44, 48, 1106, 47, 300, 65, 11, 292, 8195, 14, 33, 34326, 19, 21416, 1174, 646, 23742, 9, 1148, 742, 32, 164, 7, 386, 1576, 24, 31, 49, 13990, 4, 17, 46, 38273, 7, 14244, 1778, 1781, 34, 57, 2942, 30, 10421, 226, 16598, 354, 8, 6452, 11804, 463, 6, 34138, 225, 1722, 8, 27052, 14713, 6, 8, 598, 4399, 219, 6, 25, 157, 25, 10308, 8432, 627, 15677, 4, 20, 4732, 16, 7313, 8779, 10094, 4375, 16973, 2309, 19, 446, 2108, 1820, 1674, 8381, 12480, 8, 31563, 10153, 1509, 29648, 4, 1664, 1908, 26, 14, 4375, 45335, 6, 1778, 2593, 2425, 7, 460, 28, 3871, 7, 65, 1404, 6, 32, 55, 9, 10, 44, 48, 28825, 9562, 17, 46, 8, 4634, 197, 28, 13588, 30, 34481, 12986, 3454, 5904, 4, 50118, 50118, 30219, 3440, 5746, 189, 2703, 6477, 21416, 6927, 8, 1778, 28811, 4, 470, 589, 2589, 6020, 14909, 1766, 3823, 1581, 14, 5, 4375, 16973, 2309, 223, 3221, 473, 45, 6, 584, 6, 44, 48, 1711, 16, 10, 36298, 4683, 1174, 11206, 358, 538, 4375, 16973, 1174, 16, 13778, 19, 41, 2081, 14, 4632, 31, 1446, 11, 14, 4375, 16973, 4, 17, 46, 1190, 20925, 7623, 6, 10, 320, 12612, 6078, 7236, 8, 595, 471, 9, 714, 8, 4099, 1860, 13, 18098, 1000, 36, 1264, 9, 5, 1154, 21416, 6927, 238, 1507, 35, 44, 48, 133, 6927, 32, 5, 8751, 5924, 7, 5, 1445, 21416, 980, 6, 8, 98, 9233, 9, 106, 16, 1153, 144, 505, 4, 17, 46, 91, 3148, 124, 14, 89, 21, 117, 595, 5746, 6, 5196, 5, 7404, 13, 194, 672, 11150, 6, 215, 25, 188, 469, 17, 27, 29, 15547, 46344, 35, 44, 48, 1106, 47, 236, 7, 889, 19069, 15, 11388, 6, 13, 1246, 6, 47, 240, 10, 4385, 1174, 98, 24, 189, 45, 28, 25, 10697, 10, 1068, 4, 17, 46, 50118, 50118, 176, 4, 4307, 41603, 6390, 3858, 7981, 25049, 1723, 5758, 2606, 25815, 2118, 26747, 6597, 2571, 26116, 211, 11595, 2444, 7164, 7744, 24844, 9112, 2796, 3614, 2492, 26824, 4248, 31299, 7111, 26824, 4, 50118, 50118, 12667, 625, 18579, 6, 5, 3614, 32553, 5157, 150, 5, 12612, 6078, 32553, 9990, 8, 19069, 4, 5994, 21416, 16, 10, 573, 50, 8497, 1189, 4381, 6, 25, 1337, 2849, 11828, 19245, 9, 5, 21416, 10764, 1539, 2210, 4099, 9696, 4, 286, 4327, 6, 5, 3614, 682, 3811, 1437, 14, 1117, 430, 21416, 22121, 58, 5157, 11, 41, 6516, 1446, 403, 150, 10, 752, 1679, 3447, 14, 6229, 2593, 101, 8518, 21395, 10, 8497, 4, 50118, 50118, 17521, 1908, 373, 15, 1148, 7, 694, 10498, 15, 61, 9, 5, 2213, 126, 114, 45, 1583, 126, 9, 15750, 11, 8066, 32, 5157, 4411, 9990, 35, 44, 48, 32591, 6, 52, 17, 27, 417, 101, 7, 192, 488, 5523, 2301, 4, 17, 46, 13513, 34798, 271, 26, 5, 1762, 9, 10498, 6670, 33552, 6091, 25, 171, 21416, 12, 3368, 2975, 137, 5, 3614, 32, 44, 48, 3654, 562, 5274, 17, 46, 15, 549, 49, 785, 3594, 5157, 4, 20, 898, 16, 14, 103, 21416, 2566, 32, 44, 48, 12891, 751, 5, 315, 532, 17, 46, 7, 12982, 49, 265, 4, 3823, 13335, 6, 600, 6, 14, 9588, 814, 115, 67, 14409, 41, 7335, 14, 5, 168, 4548, 21416, 4, 264, 2449, 136, 6901, 21416, 88, 5, 13588, 23567, 13, 2490, 9, 1311, 24, 44, 48, 38731, 17022, 12360, 4, 17, 46, 50118, 50118, 250, 2472, 7, 5, 4099, 15578, 2168, 115, 28, 29002, 5, 3614, 8, 12612, 6078, 6, 61, 13513, 34798, 271, 11585, 6, 25, 33, 171, 643, 4, 1148, 6, 959, 6, 34, 2343, 410, 11037, 7, 109, 98, 576, 5, 430, 9588, 1540, 17607, 963, 4, 50118, 50118, 246, 4, 12612, 6078, 18342, 15770, 6997, 28120, 12435, 3842, 24844, 7831, 39585, 15004, 33804, 5725, 4322, 4248, 3001, 18625, 15004, 6372, 29313, 26747, 6597, 8625, 15231, 18827, 4, 50118, 17521, 1908, 585, 484, 1022, 23, 5, 12612, 6078, 148, 5, 33725, 515, 4, 1234, 6, 8250, 25388, 6078, 40, 555, 5, 1387, 9, 3777, 12469, 6, 2207, 2024, 7, 5, 3356, 17, 27, 29, 558, 4, 18775, 8697, 14267, 42, 30, 9052, 6, 44, 48, 170, 32, 375, 5, 23125, 2630, 1289, 6, 8, 1778, 1781, 8, 34930, 613, 4233, 33, 66, 20629, 49, 6255, 34341, 4, 17, 46, 4665, 6, 12612, 6078, 17, 27, 29, 1387, 9, 19458, 3061, 8, 2548, 18645, 40, 28, 588, 9044, 624, 5, 1387, 9, 1909, 4702, 6, 61, 18775, 8697, 26, 74, 44, 48, 459, 33943, 1915, 8, 10, 5153, 2969, 9, 5, 743, 2114, 5, 937, 285, 1567, 6477, 5, 144, 2008, 782, 11, 5, 144, 4478, 1822, 4, 17, 46, 8426, 16018, 5206, 624, 10, 8199, 189, 2082, 10, 28536, 34321, 53, 64, 4227, 1022, 11, 3425, 476, 6, 1218, 1056, 6, 8, 13993, 1938, 4, 10480, 352, 2207, 7, 5, 3428, 3488, 41, 558, 17, 27, 29, 3446, 8, 28602, 4, 50118, 50118, 306, 4, 3703, 4307, 41603, 6390, 83, 38798, 1862, 274, 2606, 36, 3411, 31534, 3388, 6, 83, 163, 12027, 30876, 24394, 8640, 4629, 2571, 12743, 20381, 9363, 11694, 15231, 18827, 26610, 50118, 32291, 3811, 14, 21416, 16, 44, 48, 25064, 352, 540, 5693, 8, 55, 6336, 87, 10, 55, 30574, 467, 6, 17, 46, 8, 473, 45, 33, 143, 24032, 923, 4, 18098, 1000, 17, 27, 29, 20925, 7623, 19286, 35, 44, 48, 133, 2249, 259, 19, 10607, 25, 5, 18035, 3509, 839, 30, 61, 47, 64, 2937, 923, 16, 14, 89, 32, 3668, 117, 14213, 4, 17, 46, 13513, 34798, 271, 11237, 1507, 19, 20925, 7623, 14, 44, 48, 170, 17, 27, 241, 164, 7, 33, 5, 92, 2706, 9, 1918, 1222, 1790, 8, 2381, 2154, 1634, 283, 66, 9, 42, 2682, 6, 17, 46, 53, 13335, 14, 150, 37, 21, 23, 5, 3614, 6, 44, 48, 37071, 66, 9, 2724, 646, 44675, 139, 2975, 742, 58, 12162, 3526, 6, 8, 172, 66, 9, 5, 65, 66, 9, 2724, 6, 1117, 66, 9, 2724, 9, 167, 58, 1153, 3526, 4, 17, 46, 1773, 644, 8835, 6, 81, 4059, 6, 151, 82, 33, 14332, 685, 81, 68, 134, 325, 7, 23234, 3329, 21416, 4, 50118, 50118, 11243, 1072, 7, 1877, 10, 7230, 9, 5, 2266, 720, 613, 1486, 4, 598, 109, 98, 6, 5904, 33, 2061, 15, 11473, 8, 31904, 44, 48, 19675, 636, 810, 17, 46, 7, 5, 613, 467, 4, 5937, 114, 37, 3681, 10, 44, 48, 18763, 8, 1455, 4854, 7, 5, 2210, 776, 467, 6, 17, 46, 1664, 1908, 26, 37, 222, 45, 6, 7547, 66, 14, 21416, 16, 45, 21547, 34082, 7, 7277, 18029, 810, 4, 91, 1581, 5, 7280, 11, 21416, 3266, 81, 5, 375, 484, 377, 222, 45, 1303, 910, 39453, 11, 5, 613, 467, 50, 5, 5153, 866, 4, 13513, 34798, 271, 1224, 5, 864, 9, 18029, 810, 124, 2500, 5, 2163, 9, 613, 5904, 1996, 35, 44, 48, 2264, 16, 18029, 810, 116, 1437, 85, 17, 27, 29, 5, 810, 14, 10, 752, 714, 5406, 16, 164, 7, 4852, 66, 10, 827, 6, 1169, 2024, 50, 20826, 4, 17, 46, 3823, 1507, 14, 741, 8459, 66, 21416, 74, 28, 10, 5021, 2677, 11813, 35, 44, 48, 1106, 932, 197, 28, 441, 7, 5998, 6, 24, 197, 28, 21416, 6, 61, 965, 17, 27, 90, 1174, 1435, 8737, 776, 2148, 4, 17, 46, 50118, 50118, 32291, 67, 1581, 5, 37015, 11, 7576, 14889, 15, 470, 720, 17755, 61, 7715, 30328, 5746, 13, 19069, 35, 44, 48, 243, 17, 27, 29, 818, 14085, 7, 5, 8770, 52, 794, 198, 24927, 11, 5, 4525, 29, 4, 17, 46, 3560, 6814, 24927, 6, 101, 21416, 122, 6, 2713, 7082, 5746, 8, 3284, 1147, 2423, 5, 2849, 28752, 5501, 1486, 4, 18775, 8697, 1581, 14, 65, 9, 2266, 17, 27, 29, 934, 7079, 21, 5, 240, 13, 5, 12612, 6078, 7, 3720, 210, 7218, 11, 5, 44, 48, 3293, 347, 646, 2137, 12, 627, 12, 24774, 742, 30666, 980, 4, 17, 46, 27159, 27463, 477, 7, 5, 7482, 806, 25, 145, 22646, 55, 8818, 6, 150, 4853, 477, 7, 5, 1762, 9, 2969, 9, 5894, 9, 5, 210, 6, 215, 25, 99, 1781, 124, 4375, 45335, 101, 255, 17517, 4, 50118, 50118, 245, 4, 30540, 4307, 41603, 6390, 18012, 4629, 24199, 20381, 9363, 11694, 2808, 22065, 7744, 116, 50118, 44623, 1975, 37079, 27463, 5705, 22884, 613, 9290, 25, 10, 538, 1796, 14135, 5, 723, 9453, 9, 2719, 8, 1822, 9, 3195, 54, 33, 723, 1162, 9, 145, 28363, 22697, 50, 223, 428, 22697, 30, 2065, 2879, 4, 3823, 13335, 136, 44, 48, 37466, 5257, 9290, 17, 46, 7594, 14, 6, 44, 48, 10105, 89, 17, 27, 29, 117, 8737, 2148, 639, 106, 6, 49, 923, 38618, 31, 2609, 951, 1493, 7, 907, 106, 31, 47, 4, 17, 46, 20925, 7623, 17, 27, 29, 2334, 6, 29354, 39, 676, 2754, 25, 10, 12612, 6078, 4589, 19, 39, 86, 11, 5, 21416, 539, 35, 44, 48, 7605, 127, 308, 676, 1174, 23, 5, 12612, 6078, 6, 89, 17, 27, 29, 2710, 9, 3446, 14, 17, 27, 29, 416, 11, 317, 13, 5, 1218, 7, 1174, 28, 1256, 16801, 8, 3487, 9383, 26083, 2088, 6, 190, 11, 1110, 9, 99, 888, 197, 28, 4638, 7, 6, 1605, 6, 2304, 867, 6, 50, 1434, 9, 10, 1761, 215, 25, 18098, 1000, 4, 17, 46, 91, 3811, 14, 5, 235, 714, 16, 44, 48, 18116, 82, 5, 945, 7, 28, 963, 8, 3754, 11, 5, 980, 14, 51, 101, 53, 442, 686, 14, 24, 17, 27, 29, 626, 19, 5, 235, 25563, 4, 17, 46, 50140], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode_plus(text, add_special_tokens=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1758"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50118, 134, 4, 289, 5969, 21770, 46058, 104, 3243, 248]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start = 0\n",
      "end = 512\n",
      "start = 512\n",
      "end = 1024\n",
      "start = 1024\n",
      "end = 1536\n",
      "start = 1536\n",
      "end = 1758\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "window_length = 512\n",
    "\n",
    "total_len = len(input_ids)\n",
    "\n",
    "loop = True\n",
    "\n",
    "while loop:\n",
    "    end = start + window_length\n",
    "    if end >= total_len:\n",
    "        loop = False\n",
    "        end = total_len\n",
    "    print(f\"start = {start}\")\n",
    "    print(f\"end = {end}\")\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_to_window_size_and_predict_proba(input_ids, attention_mask, total_len):\n",
    "\n",
    "    proba_list = []\n",
    "    start = 0\n",
    "    window_length = 510\n",
    "    loop = True\n",
    "\n",
    "    while loop:\n",
    "        end = start + window_length\n",
    "\n",
    "        if end >= total_len:\n",
    "            loop = False\n",
    "            end = total_len\n",
    "            \n",
    "        # Define the text chunk\n",
    "        input_ids_chunk = input_ids[start:end]\n",
    "        attention_mask_chunk = attention_mask[start:end]\n",
    "\n",
    "        # Append [CLS] and [SEP]\n",
    "        input_ids_chunk = [101] + input_ids_chunk + [102]\n",
    "        attention_mask_chunk = [1] + attention_mask_chunk + [1]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        input_dict = {\n",
    "            'input_ids': torch.Tensor([input_ids_chunk]).long(),\n",
    "            'attention_mask': torch.Tensor([attention_mask_chunk]).int()\n",
    "        }\n",
    "\n",
    "        outputs = model(**input_dict)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "        proba_list.append(probabilities)\n",
    "\n",
    "        start = end  # Update start for the next chunk\n",
    "\n",
    "    return proba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_list = chunk_text_to_window_size_and_predict_proba(input_ids, attention_mask, total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[6.4013e-04, 3.0657e-01, 6.9279e-01]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0471, 0.9515, 0.0014]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3914, 0.6037, 0.0050]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0006, 0.4391, 0.5603]], grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.4013e-04, 3.0657e-01, 6.9279e-01]],\n",
       "\n",
       "        [[4.7112e-02, 9.5149e-01, 1.4015e-03]],\n",
       "\n",
       "        [[3.9138e-01, 6.0366e-01, 4.9529e-03]],\n",
       "\n",
       "        [[6.1176e-04, 4.3910e-01, 5.6029e-01]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacks = torch.stack(proba_list)\n",
    "stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = stacks.shape\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.4013e-04, 3.0657e-01, 6.9279e-01],\n",
       "        [4.7112e-02, 9.5149e-01, 1.4015e-03],\n",
       "        [3.9138e-01, 6.0366e-01, 4.9529e-03],\n",
       "        [6.1176e-04, 4.3910e-01, 5.6029e-01]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(stacks, (shape[0], shape[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:868: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "def get_mean_from_proba(proba_list):\n",
    "    \"\"\"\n",
    "    This function computes the mean probabilities of class predictions over all the chunks.\n",
    "\n",
    "    Args:\n",
    "        proba_list (List[torch.Tensor]): List of probability tensors for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        mean (torch.Tensor): Mean of the probabilities across all chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensures that gradients are not computed, saving memory\n",
    "    with torch.no_grad():\n",
    "        # Stack the list of tensors into a single tensor\n",
    "        stacks = torch.stack(proba_list)\n",
    "\n",
    "        # Resize the tensor to match the dimensions needed for mean computation\n",
    "        stacks = stacks.resize(stacks.shape[0], stacks.shape[2])\n",
    "\n",
    "        # Compute the mean along the zeroth dimension (i.e., the chunk dimension)\n",
    "        mean = stacks.mean(dim = 0)\n",
    "        \n",
    "    return mean\n",
    "\n",
    "mean = get_mean_from_proba(proba_list)\n",
    "# tensor([0.0767, 0.1188, 0.8045])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(mean).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'C:\\Users\\hp\\Desktop\\lars\\project\\module 5\\final_scores.csv'\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print('Final Scores file saved to Module 5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
